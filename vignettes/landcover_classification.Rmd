---
title: "landcover_classification"
author: "Manuel Kunz"
date: "2023-10-26"
output: html_document
---

### unsupervised classification with the k-method

```{r library, include=FALSE}
library(terra)
library(dplyr)
library(ggplot2)
library(patchwork)
library(MODISTools)
library(appeears)
library(vroom)
library(rsample)
library(parsnip)
library(workflows)
library(tune)
library(dials)
library(xgboost)
```

```{r, download LAI data}
# Download a larger data cube
# note that I sample a 100x100 km
# area around the lat/lon location
lai_2012 <- MODISTools::mt_subset(
  product = "MCD15A3H",
  lat = 46.6756,
  lon = 7.85480,
  band = "Lai_500m",
  start = "2012-01-01",
  end = "2012-12-31",
  km_lr = 100,
  km_ab = 100,
  site_name = "swiss",
  internal = TRUE,
  progress = TRUE
)

saveRDS(lai_2012, (paste0(here::here(),"./data/lai_2012.rds")))

#readRDS("C:/Studium/AGDS_II/handfull_of_pixels/data/lai_2012.rds")

# save this data for later use
# to speed up computation
```

```{r, convert to raster}
# conversion from tidy data to a raster format
# as it is common to use raster data
r <- MODISTools::mt_to_terra(
  lai_2012,
  reproject = TRUE
  )
```

```{r, convert data to a dataframe}
# convert a multi-layer raster image
# to wide dataframe
df <- as.data.frame(r, cell = TRUE)

# the content of a single feature (vector)
# limited to the first 5 values for brevity
print(df[1,1:5])
```

```{r, cluster data}
# cluster the data 
clusters <- kmeans(
  df[,-1],
  centers = 4
)
```

```{r, map cluster classification}
# use the original raster layout as
# a template for the new map (only
# using a single layer)
kmeans_map <- terra::rast(r, nlyr=1)

# assign to each cell value (location) of this
# new map using the previously exported cell
# values (NA values are omitted so a 1:1
# mapping would not work)
kmeans_map[df$cell] <- clusters$cluster
```

```{r, plot map}
library(leaflet)

# set te colour scale manually
palcol <- colorFactor(
  c("#78d203", "#f9ffa4", "#5F9EA0", "#EEE0E5"),
  domain = 1:4,
  na.color = "transparent"
  )

# build the leaflet map
leaflet() |> 
  addProviderTiles(providers$Esri.WorldImagery, group = "World Imagery") |>
  addProviderTiles(providers$Esri.WorldTopoMap, group = "World Topo") |>
  addRasterImage(
    kmeans_map,
    colors = palcol,
    opacity = 0.5,
    method = "ngb",
    group = "k-means cluster results"
    ) |>
  addLayersControl(
    baseGroups = c("World Imagery","World Topo"),
    position = "topleft",
    options = layersControlOptions(collapsed = FALSE),
    overlayGroups = c("k-means cluster results")
    ) |>
  addLegend(
    colors = palcol(1:4),
    values = c(1, 2, 3, 4),
    title = "cluster",
    labels = c(1, 2, 3, 4)
    )
```

### supervised classification

```{r, download validation site data}
# Read the validation sites from
# Fritz et al. 2017 straight from
# Zenodo.org
validation_sites <- readr::read_csv(
  "https://zenodo.org/record/6572482/files/Global%20LULC%20reference%20data%20.csv?download=1"
)
```

```{r, filter validation sites}
# filter out data by competition,
# coverage percentage and latitude
# (use round brackets to enclose complex
# logical statements in a filter call!)
validation_selection <- validation_sites |>
    dplyr::filter(
      (competition == 4 | competition == 1),
      perc1 > 80,
      lat > 0
    )

# the above selection includes all data
# but we now subsample to 150 random locations
# per (group_by()) land cover class (LC1)
# set a seed for reproducibilty
set.seed(0)

validation_selection <- validation_selection |>
    dplyr::slice_sample(n = 150, by = LC1)

# split validation selection
# by land cover type into a nested
# list, for easier processing
# later on
validation_selection <- validation_selection |>
    dplyr::group_by(LC1) |>
    dplyr::group_split()
```

```{r}
# for every row download the data for this
# location and the specified reflectance
# bands
task_nbar <- lapply(validation_selection, function(x){
  
  # loop over all list items (i.e. land cover classes)
  base_query <- x |>
    dplyr::rowwise() |>
    do({
      data.frame(
        task = paste0("nbar_lc_",.$LC1),
        subtask = as.character(.$pixelID),
        latitude = .$lat,
        longitude = .$lon,
        start = "2012-01-01",
        end = "2012-12-31",
        product = "MCD43A4.061",
        layer = paste0("Nadir_Reflectance_Band", 1:4)
      )
    }) |>
    dplyr::ungroup()
  
  # build a task JSON string 
  task <- rs_build_task(
    df = base_query
  )
  
  # return task
  return(task)
})

# Query the appeears API and process
# data in batches - this function
# requires an active API session/login
rs_request_batch(
  request = task_nbar,
  workers = 10,
  user = "earth_data_manuel",
  path = "C:/Studium/AGDS_II/landcover_classification/data",
  verbose = TRUE,
  time_out = 28800
)

dir_data <- "C:/Studium/AGDS_II/landcover_classification/data"
```

```{r}
# list all MCD43A4 files, note that
# that list.files() uses regular
# expressions when using wildcards
# such as *, you can convert general
# wildcard use to regular expressions
# with glob2rx()
files <- list.files(
  dir_data,
  glob2rx("*MCD43A4-061-results*"),
  recursive = TRUE,
  full.names = TRUE
)

# read in the data (very fast)
# with {vroom} and set all
# fill values (>=32767) to NA
nbar <- vroom::vroom(files)
nbar[nbar >= 32767] <- NA

# retain the required data only
# and convert to a wide format
nbar_wide <- nbar |>
  dplyr::select(
    Category,
    ID,
    Date,
    Latitude,
    Longitude,
    starts_with("MCD43A4_061_Nadir")
  ) |>
  tidyr::pivot_wider(
    values_from = starts_with("MCD43A4_061_Nadir"),
    names_from = Date
  )

# split out only the site name,
# and land cover class from the
# selection of validation sites
# (this is a nested list so we
# bind_rows across the list)
sites <- validation_selection |>
  dplyr::bind_rows() |>
  dplyr::select(
    pixelID,
    LC1
  ) |>
  dplyr::rename(
    Category = "pixelID"
  )

saveRDS(sites, (paste0(here::here(),"./data/validation_site_selection.rds")))

# combine the NBAR and land-use
# land-cover labels by location
# id (Category)
ml_df <- left_join(nbar_wide, sites) |>
    dplyr::select(
    LC1,
    contains("band")
  )
```

```{r}
# create a data split across
# land cover classes
ml_df_split <- ml_df |>
  rsample::initial_split(
  strata = LC1,
  prop = 0.8
)

# select training and testing
# data based on this split
train <- rsample::training(ml_df_split)
test <- rsample::testing(ml_df_split)

saveRDS(train, (paste0(here::here(),"./data/train_data.rds")))
saveRDS(test, (paste0(here::here(),"./data/test_data.rds")))
```

```{r, model setup and training}
# specify our model structure
# the model to be used and the
# type of task we want to evaluate
model_settings <- parsnip::boost_tree(
  trees = 50,
  min_n = tune(),
  tree_depth = tune(),
  # learn_rate = tune()
  ) |>
  set_engine("xgboost") |>
  set_mode("classification")

# create a workflow compatible with
# the {tune} package which combines
# model settings with the desired
# model structure (data / formula)
xgb_workflow <- workflows::workflow() |>
  add_formula(as.factor(LC1) ~ .) |>
  add_model(model_settings)

print(xgb_workflow)
```

```{r, hyperparameter settings}
# load the dials package
# responsible for (hyper) parameter
# sampling schemes to tune
# parameters (as extracted)
# from the model specifications

hp_settings <- dials::grid_latin_hypercube(
  tune::extract_parameter_set_dials(xgb_workflow),
  size = 3
)

print(hp_settings)
```

```{r}
# set the folds (division into different)
# cross-validation training datasets
folds <- rsample::vfold_cv(train, v = 3)

# optimize the model (hyper) parameters
# using the:
# 1. workflow (i.e. model)
# 2. the cross-validation across training data
# 3. the (hyper) parameter specifications
# all data are saved for evaluation
library(foreach)
library(parallel)
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

library(xgboost)
xgb_results <- tune::tune_grid(
  xgb_workflow,
  resamples = folds,
  grid = hp_settings,
  control = tune::control_grid(save_pred = TRUE)
)
readRDS(paste0(here::here(),"./data/xgb_results.rds"))
saveRDS(xgb_results, (paste0(here::here(),"./data/xgb_results.rds")))

```

```{r, select best model}

# select the best model based upon
# the root mean squared error
xgb_best <- tune::select_best(
  xgb_results,
  metric = "roc_auc"
  )

# cook up a model using finalize_workflow
# which takes workflow (model) specifications
# and combines it with optimal model
# parameters into a model workflow
xgb_best_hp <- tune::finalize_workflow(
  xgb_workflow,
  xgb_best
)

print(xgb_best_hp)
```

```{r}
# train a final (best) model with optimal
# hyper-parameters
xgb_best_model <- fit(xgb_best_hp, train)

# run the model on our test data
# using predict()
test_results <- predict(xgb_best_model, test)

# load the caret library to
# access confusionMatrix functionality
library(caret)


# use caret's confusionMatrix function to get
# a full overview of metrics
caret::confusionMatrix(
  reference = as.factor(test$LC1),
  data = as.factor(test_results$.pred_class)
  )
```

```{r}
# We can define an appeears
# download task using a simple
# dataframe and a map from which
# an extent is extracted
task_df <- data.frame(
  task = "raster_download",
  subtask = "swiss",
  start = "2012-01-01",
  end = "2012-12-31",
  product = "MCD43A4.061",
  layer = paste0("Nadir_Reflectance_Band", 1:4)
)

# build the area based request/task
# using the extent of our previous
# kmeans map, export all results
# as geotiff (rather than netcdf)
task <- rs_build_task(
  df = task_df,
  roi = kmeans_map,
  format = "geotiff"
)

# request the task to be executed
# with results stored in a
# temporary location (can be changed)
rs_request(
  request = task,
  user = "your_api_id",
  transfer = TRUE,
  path = tempdir(),
  verbose = TRUE
)
```

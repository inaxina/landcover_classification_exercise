---
title: "Land-Use and Land-Cover Classification Exercise - Final Report"
author: "Manuel Kunz"
date: "2024-01-08"
bibliography: "references.bib"
output: html_document
---

## Supervised machine learning

In this report, I present my results from exercise 8.3 "Land-Use and Land-Cover modelling" of Block 4 "Land cover classification" from the course "Applied Geodata Science II". In the first chapter (1), I will give a short summary about the model used in this exercise, which was already set up in Chapter 7 "Land-Cover classification" of the [Handfull of Pixels book](https://geco-bern.github.io/handfull_of_pixels/land_cover_classification.html). Afterwards, Chapter 2 contains the results to exercise 8.3. Finally, I will shortly summarise the my personal most important learnings and findings of this exercise in Chapter 3.

### 1 Land-Use and Land-Cover (LULC) model - Set up
#### 1.1 Validation data
In this exercise, I will use a random forest machine learning algorithm to classify satellite images. First, I downloaded a validation data set, containing land cover and land use reference data from @Fritz2017. The original data set contains over 150'000 observations, which is why the data was reduced. From the reduced data set, I selected 150 random location per LULC class (there are 10 LULC classes meaning I selected a total of 1'500 locations). Figure 1 shows the spatial distribution of the selected validation data as well as their LULC class.
```{r, data preparation, message=FALSE, warning=FALSE}
validation_sites <- readRDS(here::here("./data/validation_sites.RDS"))
# filter out data by competition, coverage, percentage and latitude
validation_selection <- validation_sites |>
  dplyr::filter(
    (competition == 4 | competition == 1),
    perc1 > 80,
    lat > 0
  )

set.seed(1)
validation_selection <- validation_selection |>
  dplyr::slice_sample(n = 150, by = LC1)

```



```{r, validation sites, echo=FALSE, message=FALSE, warning=FALSE, fig.cap= "Figure 1: Map showing the chosen validation sites classified in 10 LULC classes"}
library(here)
validation_selection <- readRDS(here::here("./data/validation_selection.rds"))

library(leaflet)
palcol <- colorFactor(
  c('#8dd3c7','#ffffb3','#bebada','#fb8072','#80b1d3','#fdb462','#b3de69','#fccde5','#d9d9d9','#bc80bd'),
  domain = 1:10,
  na.color = "transparent"
)

leaflet() |>
  addProviderTiles(providers$Esri.WorldImagery, group = "World Imagery") |>
  addProviderTiles(providers$Esri.WorldTopoMap, group = "World Topo") |>
  addCircleMarkers(
    data = validation_selection,
    lng = ~lon,
    lat = ~lat,
    color = ~palcol(LC1),
    radius = 0.05,
    opacity = 1,
    fillOpacity = 1,
    group = "Validation sites"
  ) |>
  addLayersControl(
    baseGroups = c("World Imagery","World Topo"),
    position = "topleft",
    options = layersControlOptions(collapsed = FALSE),
    overlayGroups = c("Validation sites")
  ) |>
  addLegend(
    colors = palcol(1:10),
    values = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
    title = "LULC classes",
    labels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
  )
```

#### 1.2 Training data
Now that the validation sites are sampled, I downloaded daily multi-spectral data from the MCD43A4 product for 2012 using the {appears} package in R as training data. The training data was then prepared to be used in the model training.

#### 1.3 Model training
First, the data was split into training and testing data splits using the {rsample} package. To create the model, the {parsnip} R package was used. Ultimately, the model was defined as following: 
```{r, model sturcture, echo=FALSE, message=FALSE, warning=FALSE}
library(parsnip)
model_settings <-readRDS(here::here("./data/model_settings.rds"))
print(model_settings)

```
The following workflow was used ({tune} R package). In the first calcution step, only tree size were defined as hyperparameters. 
```{r, workflow, echo=FALSE, message=FALSE, warning=FALSE}
library(tune)
xgb_workflow <- readRDS(here::here("./data/xgb_workflow.rds"))
print(xgb_workflow)
```
As visible in the workflow, min_n and tree_depth were not defined. This is because these hyperparameters were optimised using the {tune} and {dials} packages.

```{r, hyperparameter tuning,echo=FALSE, message=FALSE, warning=FALSE}

hp_settings <- readRDS(here::here("./data/hp_settings.rds"))
print(hp_settings)
```

#### 1.4 Model calibration
The model was calibrated using a 3-fold cross validation. Then the model was optimised across the folds to find the best hyperparameter settings. Once the best hyperparameter setting was found, the final model was created using the "fit" fuction. The best hyperparameter settings for the first calcualtion are the following:
```{r, hyperparameter setting, echo=FALSE, message=FALSE, warning=FALSE}
readRDS(here::here("./data/xgb_best_hp.rds"))
```
#### 1.5 Model evaluation
Once the hyperparameter were set, the predict function was used to run the model with the testing data. Finally, the results of the evaluation are presented in the following confusion matrix:
```{r, confusion matrix, echo=FALSE, message=FALSE, warning=FALSE}
library(caret)
readRDS(here::here("./data/confusion_matrix.rds"))
```

### 2 Exercises
#### 2.1 Four ways to improve the LULC model
A first way to improve the model would be to increase the size of the validation data. As described in Chapter 1.1, I randomly selected 150 validation location per LULC class. If the number of validation locations is increased, the probability that the validation data covers a larger area is increased.

A second way to improve the model is to apply a different cross-validation strategy. As described in Chapter 1.4, I used a random cross validation strategy to train the model. By applying a spatial cross-validation, where the folds are created in a way that accounts for the geographical distribution of the data, the model might be improved.

Thirdly, the model could be improved by tuning additional hyperparameters. In the presented workflow (Chapter 1.3), the hyperparameter "number of trees" was fixed at 50, and min_n and tree_depth were tuned. In order to improve the LULC model, the number of trees can be change, or additional hyperparameters (e.g. mtry "The number of variables to consider to make decisions at each node". Definition taken from AGDS I, Chapter 11.3) can be included.

Lastly, a fourth way to improve the LULC model is to increase the number of folds in the cross-validation. In the base model, the number of folds was set to 3. Increasing the fold number could potentially increase the model's performance.

#### 2.2 Two ways to improve the LULC model - Implementation
In this chapter, I present my implementation of two of the above-mentioned ways to improve the LULC model.

##### 2.2.1 Hyperparameter settings
As outlined in Chapter 2.1, changing the hyperparameter settings can improve the model. Therefore, I implemented the following changes to the hyperparameter settings of the model: I left the parameter "number of trees" flexible, so that it was also tuned. Furthermore, I defined the hyperparameter "mtry" as 2. Additionally, I increased the number of parameters, which are generated with the function "extract_parameters_set_dials" from the {tune} package from 5 to 10. Through that, I increase the number of possible hyperparameter settings that are tested which increases the possibility to find a better combination. The following hyperparameter setting were used:
```{r, changed hyperparameter settings, echo=FALSE, message=FALSE, warning=FALSE}
readRDS(here::here("./data/xgb_best_hp_improved.rds"))
```
Looking at the confusion matrix stats of the improved model shows, that the precision as well as Kappa was increased compared to the results of the first model configuration.
```{r, improved model statistics I, echo=FALSE, message=FALSE, warning=FALSE}
readRDS(here::here("./data/confusion_matrix_improved.rds"))
```
##### 2.2.2 10-fold cross-validation
Here, I used the same hyperparameter settings as in Chapter 2.2.1, but increased the fold size from 3 to 10. With that, I expect that the results of the model improve. The results from the second improvement are the following:
```{r, improved model statistics II, echo=FALSE, message=FALSE, warning=FALSE}
readRDS(here::here("./data/confusion_matrix_improved_II.rds"))
```
#### 2.3 Leaderboard submission
I used my improved model (final model with both improvements) to predict the LULC classes for training and testing data from @hufkens_2023_8298491. The predicted LULC results are saved in csv-file and submitted to the [agds2_course_repository](https://github.com/geco-bern/agds2_course).

### 3 Conclusion
In this exercise, I implemented a Random Forest model to predict LULC classes with real LULC data as validation data and remote sensing data as training data. I took the model that was created in Chapter 7 "Land-Cover classification" of the [Handfull of Pixels book](https://geco-bern.github.io/handfull_of_pixels/land_cover_classification.html) and improved the model by tuning the hyperparameters and increasing the number of cross-validation folds. The resulting confusion matrices showed, that the accuracy and the kappa of the predicition was actually increased compared to the base model. However, the second improvement (increasing the fold number) lead to very similar results compared to the first improvement.

### 4 References

